title: 一张图说明白AlphaGo的原理
date: 2016-05-07 13:40:41
tags:
- 机器学习
- 算法
categories:
- 机器学习
---

![](/images/2016/alpha_go_theroy.jpg)

AlphaGo总体上包含离线学习（图1上半部分）和在线对弈（图1下半部分）两个过程。

离线学习过程分为三个训练阶段。
第一阶段：利用3万多幅专业棋手对局的棋谱来训练两个网络。一个是基于全局特征和深度卷积网络（CNN）训练出来的策略网络（Policy Network）。其主要作用是给定当前盘面状态作为输入，输出下一步棋在棋盘其它空地上的落子概率。另一个是利用局部特征和线性模型训练出来的快速走棋策略（Rollout Policy)。策略网络速度较慢，但精度较高；快速走棋策略反之。

第二阶段：利用第t轮的策略网络与先前训练好的策略网络互相对弈，利用增强式学习来修正第t轮的策略网络的参数，最终得到增强的策略网络。这部分被很多“砖”家极大的鼓吹，但实际上应该存在理论上的瓶颈（提升能力有限）。这就好比2个6岁的小孩不断对弈，其水平就会达到职业9段？

第三阶段：先利用普通的策略网络来生成棋局的前U-1步（U是一个属于[1, 450]的随机变量），然后利用随机采样来决定第U步的位置（这是为了增加棋的多样性，防止过拟合）。随后，利用增强的策略网络来完成后面的自我对弈过程，直至棋局结束分出胜负。此后，第U步的盘面作为特征输入，胜负作为label，学习一个价值网络（Value Network），用于判断结果的输赢概率。价值网络其实是AlphaGo的一大创新，围棋最为困难的就是很难根据当前的局势来判断最后的结果，这点职业棋手也很难掌握。通过大量的自我对弈，AlphaGo产生了3000万盘棋局，用作训练学习价值网络。但由于为其的搜索空间太大，3000万盘棋局也不能帮AlphaGo完全攻克这个问题。

在线对弈过程包括以下5个关键步骤：其核心思想实在蒙特卡洛搜索树（MCTS）中嵌入了深度神经网络来减少搜索空间。AlphaGo并没有具备真正的思维能力。

根据当前盘面已经落子的情况提取相应特征；
利用策略网络估计出棋盘其他空地的落子概率；
根据落子概率来计算此处往下发展的权重，初始值为落子概率本身（如0.18）。实际情况可能是一个以概率值为输入的函数，此处为了理解简便。
利用价值网络和快速走棋网络分别判断局势，两个局势得分相加为此处最后走棋获胜的得分。这里使用快速走棋策略是一个用速度来换取量的方法，从被判断的位置出发，快速行棋至最后，每一次行棋结束后都会有个输赢结果，然后综合统计这个节点对应的胜率。而价值网络只要根据当前的状态便可直接评估出最后的结果。两者各有优缺点、互补。
利用第四步计算的得分来更新之前那个走棋位置的权重（如从0.18变成了0.12）；此后，从权重最大的0.15那条边开始继续搜索和更新。这些权重的更新过程应该是可以并行的。当某个节点的被访问次数超过了一定的门限值，则在蒙特卡罗树上进一步展开下一级别的搜索（如图2所示）。

![](/images/2016/alphago_MCTS.jpg)
图2、MCTS拓展下一级节点

AlphaGo的弱点在哪里？
攻其策略网络，加大搜索空间。进入中盘后，职业选手如能建立起比较复杂的局面，每一步棋都牵连很多个局部棋的命运（避免单块、局部作战），则AlphaGo需要搜索空间则急剧加大，短时间内得到的解的精度就会大打折扣。李世石九段的第四局棋就有这个意思。此处左右上下共5块黑白棋都相互关联到一起，白1下后，黑棋需要考虑很多地方。很多地方都需要在MCTS上进行跟深入的搜索。为了在一定的时间内有结果，只能放弃搜索精度。

![](/images/2016/lishishi_alphago_4.jpg)
图3、李世石对AlphaGo第四盘棋棋谱

攻其价值网络，万劫不复：AlphaGo的价值网络极大的提高了之前单纯依靠MCTS来做局势判断的精度，但离准确判断围棋局势还有不小的差距。神经网络还不能完全避免在某些时候出现一些怪异（甚至错误）的判断，更何况其训练样本还远远不足。这也是为什么有了价值网络还仍然需要依靠快速走棋来判断局势。大家都曾经怀疑过AlphaGo的打劫能力，也感觉到了AlphaGo有躲避打劫的迹象。实际上南京大学的周志华教授曾经撰文指出打劫会让价值网络崩溃的问题，原理不再重复。总之打劫要乘早，太晚了搜索空间变小，即便价值网络失效，还可以靠快速走棋网络来弥补。开劫应该以在刚刚进入中盘时期为好（太早劫财还不够），并切保持长时间不消劫，最好在盘面上能同时有两处以上打劫。没有了价值网络的AlphaGo其实水平也就职业3段左右。