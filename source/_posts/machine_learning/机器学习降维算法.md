title: 机器学习降维算法
date: 2016-05-07 15:18:49
tags:
- 机器学习
- 算法
categories:
- IT技术
- 机器学习
---

### [主成分分析算法 (Principal components analysis, PCA)](https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)
一种分析、简化数据集的技术。主成分分析经常用于减少数据集的维数，同时保持数据集中的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。但是，这也不是一定的，要视具体应用而定。由于主成分分析依赖所给数据，所以数据的准确性对分析结果影响很大。

![](/images/2016/pca_principal_component_analysis.png)
> 在PCA中，我们将每个样本看做特征线性空间中的一个向量，左图代表具有三个特征的样本（处于三维空间中，每个特征代表一个维度），通过寻找空间中样本的主成分PC1、PC2，以此建立新的二维线性空间来完成3D到2D的降维。

[在使用scikit-learn实现PCA算法](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)

#### Ng机器学习课程的例子

这次Ng的实验中展示了一个使用PCA作为人脸检测预处理的例子，使用的人脸数据看上去是这样的：
![](/images/2016/pca_example_face1.jpg)
每一个人脸图像的分辨率为36*36，每个像素作为一个灰度特征值则我们将每个人脸图片作为一个1024维的样本进行处理，进行PCA降维至100维，映射回原始空间进行可视化得到了下面的效果：
![](/images/2016/pca_example_face2.jpg)
可以观察到非常有趣的结果，虽然经过PCA数据维度降低了非常多，但是人脸的五官还是基本保留了下来（这也形象的说明了“主成分”的意义），对后续的学习算法是一个很好的加速。

### LDA（Linear Discriminant Analysis）
### LLE (Locally Linear Embedding) 局部线性嵌入
